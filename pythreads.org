* Primitives
  Concurrency Primitives provided by OS are *Processes* and *Threads*
   In Linux clone() system call is used to create both process and threads; the
   function is used to invoke with different options for each.
   A new thread will not only share the memory of its parent process, but also
   share file descriptors, file system context and signal handling.
** Two Approaches to Concurrent Programming
   1. Shared Memory
      The program is designed for two or more threads to read from and write to
      same block of memory.
   2. Message Passing
      In message passing approach any chunk of memory is only accessed by single
      thread. When two or more threads need to synchronize, interact or
      communicate, data is sent from one another.
* Process Virtual Machines OR Application Virtual Machine OR Managed Run Time Environment
  A process VM runs as normal application inside a host OS and supports a single
  process It is created when program is started and destroyed when it exits. Its
  purpose is to provide a platform independent programming environment that
  abstracts away the details of the underlying hardware or operating system. and
  allows program to execute in the same way on any platform.
  *Process VMs are implemented using an interpreter*; performance comparable to
  compiled programming languages is achieved by the use of just in time
  compilation 
  Process Virtual Machine are designed to execute a single computer program by
  providing an abstract and platform independent program execution environment.
* Stack Machines
  In computer science, computer engineering and in programming language
  implementations a *stack machine* is a real or emulated computer that uses a
  pushdown stack rather than individual registers to evaluate each subexpression
  in the program. A stack computer is programmed with a reverse Polish notation
  instruction set.
* Global Interpreter Lock (GIL)
  GIL is an interpreter-level lock. This lock prevents the execution of multiple
  threads at once in Python Interpreter process.Each thread that wants to run
  must wait for the GIL to be released by the other thread.
  CPython uses what's called the *operating system threads* under the covers,
  which is to say each time new request to make a thread is made, the
  interpreter actually calls operating system libraries and kernel to generate a
  new thread (same as Java). So in memory you have multiple threads and normally
  operating system controls which thread is scheduled to run. On a multi
  processor machine you could have many threads spread across multiple
  processors.
  However CPython does use OS threads, the interpreter also *forces GIL* to be
  acquired by a thread before it can access the interpreter stack and can modify
  the mutable objects.  The GIL prevents simultaneous access to Python objects
  in interpreter stack. Even if GIL is there we still need to use some kind of
  synchronization mechanism for our purposes.
  The GIL also keeps garbage collection working.
  If we have the GIL and a thread must own it to execute with in the
  interpreter, what decides if the GIL should be released.?. The answer is byte
  code instructions.
  When a Python application is executed, it is compiled down to byte code, the
  actual instructions that the interpreter used for execution of the
  application.  A given line of a Python application might be a single byte
  code, while others, such as import statement, may ultimately expand into many
  byte codes.

  CPython when working with pure python code will force the GIL to be released
  every hundred byte code instructions. This means that if you have have complex
  line of code like a complex math function that in reality act as single byte
  code the GIL will not be released for the peroid that statement takes to run.
  
  There is an exception though: C modules! C extension modules can be built in
  such a way that they release the GIL voluntarily and do their own magic.
  Take for instance time module ("timemodule.c" in the Python source tree). The
  sleep function looks like this

  ....
  Py_BEGIN_ALLOW_THREADS
      sleep((int)secs);
  Py_END_ALLOW_THREADS

  In a C extension, the "Py_BEGIN_ALLOW_THREADS"  and "Py_END_ALLOW_THREADS"
  macros signal the interpreter and basically state "hey I am entering some
  blocking operation, here is the GIL back" and hey I am returning I need the
  GIL. This means that anything in your application that used a blocking I/O
  function (n/w socket manipulation or file manipulation) or a thread safe C
  extension (most of built in ones are) can bypass the GIL.

  Ways to accelerate GIL manipulation or avoid it.

  -call "time.sleep()" -set"sys.checkinterval()" -run Python in optimized mode
  -dump process intensive tasks to C extensions -use subprocess module to
  -execute commands. 
  
  The GIL doesn't prevent a process from running on a different processor of a
  machine. 

  The GIL is used internally to ensure that only one thread runs in the Python
  VM at a time. In general Python offers to switch among threads only between
  bytecode instructions; how frequently it switches can be set via
  *sys.setcheckinterval*. Each bytecode instruction and therefore all the C
  implementation code reached from each bytecode instruction is therefore atomic
  from the point of view of a Python program.

  Following operations are all atomic(L, L1, L2, are lists, D* are dictionaries,
  x, y are objects, i, j are ints)

  - L.append(x)
  - L1.extend(L2)
  - x = L[i]
  - x = L.pop()
  - L1[i:j] = L2
  - L.sort()
  - x = y
  - x.field = y
  - D[x] = y
  - D1.update(D2)
  - D.keys()
  These operations are not atomic
  - i = i + 1
  - L.append(L[-1)
  - L[i] = L[j]
  - D[x] = D[x] + 1
  
* Python Threads
  Consider the following Python Code

  def process_item(item):
      global counter
      # do something with item
      counter += 1

  if you call this function from more than one thread, you will find the counter
  is not necessarily accurate. It works in most of cases, but sometimes misses
  one or more items. The reason for this is that increment operation is actually
  executed in three steps; fist the interpreter fetches the current value of the
  counter, then it calculates the new value, and finally, it writes the new
  value back to the variable.
  If another thread gets control after the current thread has fetched the
  variable, it may fetch the variable, increment it, and write it back, before
  the current thread does the same thing. And since they are both seeing the
  same original value, only one item will accounted for.

  Another problem is access to *incomplete or inconsistent state *, which can
  happen if one thread is intializing or updating some non-trivial data
  structure and another thread attempts to read the structure while its being
  upated.
** Atomic Operations
   The simplest way to synchronize access to shared variables or other resources
   is to rely on atomic operations in the interpreter. An atomic opration is an
   operation that is carried out in a single execution step without any chance
   that another thread gets control.

   In general this approach only works if the shared resource consists of a
   single instance of a core data type, such as string variable, a number, or a
   list or dictionary. Here are some thread-safe operations:

   1. reading or replacing a single instance attribure
   2. reading or replacing a single gloable variable
   3. fetching an item from a list
   4. modifying a list in place (e.g. adding and item using append)
   5. fetching an item from a dictionary
   6. modifying a dictionary in place (e.g. adding an item, or calling clear
      methods)
** Locks
   Most fundamental synchronization mechanism. At any time lock can be held by
   single thread or no thread. If a thread attempts to hold a lock that's
   already held by some other thread, execution of the first thread is halted
   until the lock is released.

   But Locks doesn't care which thread is holding lock. If a thread holding a
   lock tries to reacquire same lock it will block. To rectify this we have
   re-entrant locks.
** Semaphores
** Nature of threads in Python
   The threads when we talk in general are OS threads. This is what we get when
   writing an application in C, if you call pthread_create(Linux) or
   CreateThread(Windows). It is real thread allocated and managed by operating
   system kernel. In modern Python we start a thread by creating an instance of
   threading. Thread the invoke its start() method. A started thread indeed
   allocates a separate OS thread. Two OS thread can run at the same time fully
   utilizing two CPU cores but CPython implementation doesn't allow to run two
   threads parallely.
   As I understand it, this is due to a mutex in the join() method which has the
   implication that you cannot interrupt it with KeyboardInterrupt 
